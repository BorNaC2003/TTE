{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb3840c-3efe-48f2-bba2-ab5a8656eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Essential Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # Optional: For better visualizations\n",
    "\n",
    "# 2. Machine Learning Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans, DBSCAN  # Clustering Algorithms\n",
    "from sklearn.metrics import silhouette_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler  # Data normalization for clustering\n",
    "\n",
    "# 3. Ensure Directories for Saving Models Exist\n",
    "trial_pp_dir = os.path.join(os.getcwd(), \"trial_pp\")\n",
    "trial_itt_dir = os.path.join(os.getcwd(), \"trial_itt\")\n",
    "os.makedirs(trial_pp_dir, exist_ok=True)\n",
    "os.makedirs(trial_itt_dir, exist_ok=True)\n",
    "\n",
    "# 4. Define Trial Dictionaries (If Used Later)\n",
    "trial_pp = {\"estimand\": \"PP\"}  \n",
    "trial_itt = {\"estimand\": \"ITT\"}  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb555149-4183-4ce6-9621-ecd2a972a568",
   "metadata": {},
   "source": [
    "### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83372170-8c5a-4a5f-9edd-d4a39bf8b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Dataset\n",
    "data_censored = pd.read_csv(\"data_censored.csv\")\n",
    "\n",
    "# Define Features for Clustering\n",
    "cluster_features = data_censored[[\"x1\", \"x2\", \"x3\", \"age\"]]\n",
    "\n",
    "# Standardize Features for Clustering (Important for Distance-Based Methods)\n",
    "scaler = StandardScaler()\n",
    "cluster_features_scaled = scaler.fit_transform(cluster_features)\n",
    "\n",
    "# Apply K-Means Clustering\n",
    "num_clusters = 3  # You can adjust this based on Elbow Method or other techniques\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "data_censored[\"cluster\"] = kmeans.fit_predict(cluster_features_scaled)\n",
    "\n",
    "# Compute Silhouette Score for Clustering Quality\n",
    "silhouette_avg = silhouette_score(cluster_features_scaled, data_censored[\"cluster\"])\n",
    "data_censored[\"silhouette_score\"] = silhouette_avg  # Storing overall score (not per row)\n",
    "\n",
    "# Assign Dataset to Trial Objects\n",
    "trial_pp[\"data\"] = data_censored\n",
    "trial_itt[\"data\"] = data_censored\n",
    "\n",
    "# Save Processed Data for Further Analysis\n",
    "data_censored.to_csv(\"data_censored_clustered.csv\", index=False)\n",
    "\n",
    "# Print Summary\n",
    "print(f\"Clustering Completed. Silhouette Score: {silhouette_avg:.4f}\")\n",
    "print(data_censored.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38df8ca-db33-41ca-aa23-fbc0e02714db",
   "metadata": {},
   "source": [
    "We have made several improvements to the data preparation code to enhance its efficiency and accuracy. First, we standardized the selected features (`x1`, `x2`, `x3`, and `age`) using `StandardScaler` to ensure that K-Means clustering operates effectively, as it is sensitive to varying feature scales. Next, we corrected the calculation of the silhouette score by computing a single overall score for clustering quality, rather than assigning an individual score to each row. Additionally, we saved the processed dataset as `data_censored_clustered.csv`, allowing for further analysis and reproducibility. Finally, we included a print summary to provide immediate feedback on the clustering process and the computed silhouette score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54da257-5ed4-413c-bb8f-af39a7ca557f",
   "metadata": {},
   "source": [
    "## 2. Weight Models and Calculation (With Clustering)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c31e5f-2ff4-4b53-a031-7ac0503980ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Define Features and Target for Switch Weight Model\n",
    "X_pp = data_censored[[\"age\", \"x1\", \"x3\", \"cluster\"]]\n",
    "y_pp = data_censored[\"treatment\"]\n",
    "\n",
    "# Fit Logistic Regression Model for Switch Weights\n",
    "logit_model_pp = sm.Logit(y_pp, sm.add_constant(X_pp)).fit()\n",
    "trial_pp[\"switch_weights\"] = logit_model_pp.predict(sm.add_constant(X_pp))\n",
    "\n",
    "# Define Features and Target for Censoring Weight Model\n",
    "X_censor = data_censored[[\"x2\", \"x1\", \"cluster\"]]\n",
    "y_censor = 1 - data_censored[\"censored\"]\n",
    "\n",
    "# Fit Logistic Regression Model for Censoring Weights\n",
    "logit_model_censor = sm.Logit(y_censor, sm.add_constant(X_censor)).fit()\n",
    "trial_pp[\"censor_weights\"] = logit_model_censor.predict(sm.add_constant(X_censor))\n",
    "\n",
    "# Compute Final Weights\n",
    "trial_pp[\"weights\"] = trial_pp[\"switch_weights\"] * trial_pp[\"censor_weights\"]\n",
    "\n",
    "# Print Model Summaries for Reference\n",
    "print(\"\\nSwitch Weight Model Summary:\")\n",
    "print(logit_model_pp.summary())\n",
    "\n",
    "print(\"\\nCensoring Weight Model Summary:\")\n",
    "print(logit_model_censor.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb99a6-73d3-4542-8c32-e408512a31ac",
   "metadata": {},
   "source": [
    "We have improved the weight model and calculation code by enhancing clarity, structure, and readability. First, we explicitly defined the features and target variables for both the switch weight and censoring weight models, ensuring a clear distinction between the two. We maintained proper use of logistic regression by adding a constant term (`sm.add_constant()`) to account for the intercept in the models. Additionally, we included print statements to display the model summaries, allowing us to assess model performance and interpret the estimated coefficients. Finally, we ensured that the final weights were correctly computed by multiplying the predicted switch and censoring weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76170717-03af-4468-a742-0e47617fb86e",
   "metadata": {},
   "source": [
    "## 3. Specify Outcome Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df01d6b-3f9c-44c6-a2be-eada884d139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Define Features and Target for Outcome Model\n",
    "X_outcome = sm.add_constant(data_censored[[\"x2\", \"cluster\"]])\n",
    "y_outcome = data_censored[\"outcome\"]\n",
    "\n",
    "# Fit Logistic Regression Model for Outcome\n",
    "logit_outcome_model = sm.Logit(y_outcome, X_outcome).fit()\n",
    "trial_pp[\"outcome_model\"] = logit_outcome_model.predict(X_outcome)\n",
    "\n",
    "# Compute Pseudo R² (McFadden's R²)\n",
    "pseudo_r2_original = 1 - (logit_outcome_model.llf / logit_outcome_model.llnull)\n",
    "\n",
    "# Print Model Summary and Pseudo R²\n",
    "print(\"\\nOutcome Model Summary:\")\n",
    "print(logit_outcome_model.summary())\n",
    "print(f\"\\nPseudo R² (McFadden's R²): {pseudo_r2_original:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a8724-cece-4b8d-8eee-7672fca17be7",
   "metadata": {},
   "source": [
    "## 4. Expand, Sample, and Fit MSM  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c60fe0ac-c42b-474c-8ef7-49033ec464ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Expand the dataset via bootstrapping\n",
    "trial_pp[\"expanded_data\"] = data_censored.sample(frac=1, replace=True, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Sample a subset (50%) of the expanded dataset\n",
    "trial_pp[\"sampled_data\"] = trial_pp[\"expanded_data\"].sample(frac=0.5, random_state=1234)\n",
    "\n",
    "# Apply weight trimming to avoid extreme values\n",
    "weights = trial_pp[\"weights\"].astype(float)\n",
    "trimmed_weight_threshold = np.quantile(weights, 0.99)\n",
    "weights = np.minimum(weights, trimmed_weight_threshold)\n",
    "\n",
    "# Match trimmed weights to sampled data\n",
    "sampled_indices = trial_pp[\"sampled_data\"].index\n",
    "weights_sampled = weights.loc[sampled_indices].values  # Ensure it's an array for GLM\n",
    "\n",
    "# Define MSM model using Generalized Linear Model (GLM)\n",
    "X_msm = sm.add_constant(trial_pp[\"sampled_data\"][[\"x2\"]])  # Removing 'cluster' for baseline comparison\n",
    "y_msm = trial_pp[\"sampled_data\"][\"outcome\"]\n",
    "\n",
    "# Fit MSM model with binomial family and weighted observations\n",
    "msm_model = sm.GLM(y_msm, X_msm, family=sm.families.Binomial(), freq_weights=weights_sampled).fit()\n",
    "trial_pp[\"msm_model\"] = msm_model\n",
    "\n",
    "# Compute Pseudo R² for MSM Model\n",
    "pseudo_r2_clustered = 1 - (msm_model.llf / msm_model.llnull)\n",
    "\n",
    "# Print Model Summary and Pseudo R²\n",
    "print(\"\\nMSM Model Summary:\")\n",
    "print(msm_model.summary())\n",
    "print(f\"\\nPseudo R² (McFadden's R²) for MSM Model: {pseudo_r2_clustered:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e5216-b740-4ff7-a143-8cc954c3d07a",
   "metadata": {},
   "source": [
    "## 5. Alternative Clustering & Survival Prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d29bae2-b904-4c80-9259-71bb0834b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Apply DBSCAN Clustering\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
    "data_censored[\"dbscan_cluster\"] = dbscan.fit_predict(cluster_features)\n",
    "\n",
    "# Compute Silhouette Score (only if DBSCAN has more than 1 cluster)\n",
    "if len(set(data_censored[\"dbscan_cluster\"])) > 1:\n",
    "    dbscan_silhouette = silhouette_score(cluster_features, data_censored[\"dbscan_cluster\"], metric=\"euclidean\")\n",
    "    print(f\"DBSCAN Silhouette Score: {dbscan_silhouette:.4f}\")\n",
    "else:\n",
    "    dbscan_silhouette = None\n",
    "    print(\"DBSCAN resulted in only one cluster or noise points, Silhouette Score not applicable.\")\n",
    "\n",
    "# Predict survival probabilities using MSM Model\n",
    "predict_times = np.arange(0, 11)\n",
    "X_survival = sm.add_constant(trial_pp[\"sampled_data\"][\"x2\"])\n",
    "survival_predictions = msm_model.predict(X_survival)\n",
    "\n",
    "# Display first few survival predictions\n",
    "print(\"\\nSurvival Predictions (First 10):\")\n",
    "print(survival_predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee60c04-d858-4153-8f8c-e4cf000b7127",
   "metadata": {},
   "source": [
    "## 6. Model Comparison & Performance Evaluation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1292260-feba-42de-8d73-2604c6c1b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Compute AIC/BIC for Logit Outcome Model\n",
    "original_aic = logit_outcome_model.aic\n",
    "n = len(y_outcome)  # Number of observations\n",
    "k = logit_outcome_model.df_model + 1  # Number of parameters (including intercept)\n",
    "log_likelihood = logit_outcome_model.llf\n",
    "original_bic = k * np.log(n) - 2 * log_likelihood\n",
    "\n",
    "# Compute AIC/BIC for MSM Model\n",
    "clustered_aic = msm_model.aic\n",
    "n_msm = len(y_msm)\n",
    "k_msm = msm_model.df_model + 1\n",
    "log_likelihood_msm = msm_model.llf\n",
    "clustered_bic = k_msm * np.log(n_msm) - 2 * log_likelihood_msm\n",
    "\n",
    "# Compute AUC for both models\n",
    "auc_original = roc_auc_score(y_outcome, logit_outcome_model.predict(X_outcome))\n",
    "auc_clustered = roc_auc_score(y_msm, msm_model.predict(X_msm))\n",
    "\n",
    "# Cross-Validation Setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Train Logistic Regression Models for CV Evaluation\n",
    "logit_outcome_model_sklearn = LogisticRegression(max_iter=1000, solver=\"liblinear\").fit(X_outcome, y_outcome)\n",
    "msm_model_sklearn = LogisticRegression(max_iter=1000, solver=\"liblinear\").fit(X_msm, y_msm)\n",
    "\n",
    "# Compute Cross-Validated AUC Scores\n",
    "cross_val_original = np.mean(cross_val_score(logit_outcome_model_sklearn, X_outcome, y_outcome, cv=cv, scoring=\"roc_auc\"))\n",
    "cross_val_clustered = np.mean(cross_val_score(msm_model_sklearn, X_msm, y_msm, cv=cv, scoring=\"roc_auc\"))\n",
    "\n",
    "# Print Results for Easy Comparison\n",
    "print(\"\\n🔹 Model Comparison Metrics 🔹\")\n",
    "print(f\"Logit Model AIC: {original_aic:.4f} | BIC: {original_bic:.4f}\")\n",
    "print(f\"MSM Model AIC: {clustered_aic:.4f} | BIC: {clustered_bic:.4f}\")\n",
    "print(f\"AUC (Logit Model): {auc_original:.4f} | AUC (MSM Model): {auc_clustered:.4f}\")\n",
    "print(f\"Cross-Validated AUC (Logit Model): {cross_val_original:.4f}\")\n",
    "print(f\"Cross-Validated AUC (MSM Model): {cross_val_clustered:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
